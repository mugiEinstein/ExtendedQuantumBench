# -*- coding: utf-8 -*-
"""
Fig3: Subdomain Analysis (a/b/c/d) — paper-aligned + real data

This script reproduces Fig3 using:
- Baseline results: outputs/run_ollama/quantumbench_results_qwen2.5-7b_0.csv
- New method results: outputs/run_hybrid_v4_full/quantumbench_results_qwen2.5-7b_0.csv
- Canonical labels: quantumbench/category.csv  (Subdomain_question, QuestionType)

Outputs:
- Fig3_Subdomain_Analysis_realdata_paperaligned.png  (dpi=300, bbox tight)
- Fig3_Subdomain_Analysis_realdata_paperaligned.pdf  (bbox tight, TrueType fonts embedded)

Author: (generated by ChatGPT)
"""

import numpy as np  # 用于数值计算（bootstrap、矩阵等）
import pandas as pd  # 用于读取 CSV 并做数据对齐/聚合
import matplotlib.pyplot as plt  # 主绘图库
from matplotlib.gridspec import GridSpec  # 控制 4 子图布局
import seaborn as sns  # 用于 heatmap + 调色板
import textwrap  # 用于长标签自动换行（避免重叠）

# =========================
# 0) 路径配置（你只需要改这里）
# =========================

# 【数据文件 1：baseline run 输出】来自你 zip：
# 论文复现工作/QuantumBench/outputs/run_ollama/quantumbench_results_qwen2.5-7b_0.csv
BASELINE_CSV = "论文复现工作/QuantumBench/outputs/run_ollama/quantumbench_results_qwen2.5-7b_0.csv"

# 【数据文件 2：new run 输出】来自你 zip：
# 论文复现工作/QuantumBench/outputs/run_hybrid_v4_full/quantumbench_results_qwen2.5-7b_0.csv
NEW_CSV = "论文复现工作/QuantumBench/outputs/run_hybrid_v4_full/quantumbench_results_qwen2.5-7b_0.csv"

# 【数据文件 3：题目元信息标签】来自你 zip：
# 论文复现工作/QuantumBench/quantumbench/quantumbench/category.csv
# 用它提供 Subdomain_question（规范子领域）和 QuestionType（题型）
CATEGORY_CSV = "论文复现工作/QuantumBench/quantumbench/quantumbench/category.csv"

# 【输出文件名前缀】
OUT_PREFIX = "Fig3_Subdomain_Analysis_realdata_paperaligned"

# =========================
# 1) 工具函数：标签换行（避免 x tick 文字重叠）
# =========================

def wrap_labels(labels, width=14):
    """
    将长标签自动换行，减轻 LaTeX / PDF 缩放后 x tick 重叠问题。
    - labels: list[str]
    - width: 每行最大字符数
    """
    return [textwrap.fill(str(s), width=width) for s in labels]


# =========================
# 2) 读取数据（明确每个字段来自哪里）
# =========================

# 读取 baseline 结果：使用字段
# - "Question id": 题号（用于对齐）
# - "Correct": 是否答对（用于计算准确率）
base_df = pd.read_csv(BASELINE_CSV)  # <-- 使用 BASELINE_CSV 文件

# 读取 new 结果：使用字段
# - "Question id": 题号（用于对齐）
# - "Correct": 是否答对（用于计算准确率）
new_df = pd.read_csv(NEW_CSV)  # <-- 使用 NEW_CSV 文件

# 读取 canonical 标签表：使用字段
# - "Question id": 题号（用于 merge）
# - "Subdomain_question": 规范子领域（Fig3 按子领域统计必须用它）
# - "QuestionType": 题型（Conceptual / Numerical / Algebraic）
cat_df = pd.read_csv(CATEGORY_CSV)  # <-- 使用 CATEGORY_CSV 文件

# 只保留我们需要的列（避免无关列干扰）
cat_df = cat_df[["Question id", "Subdomain_question", "QuestionType"]].copy()  # <-- CATEGORY_CSV: 这三列


# =========================
# 3) 对齐 baseline 与 new（严格按 Question id 配对）
# =========================

# baseline 只留：Question id + Correct
base_keep = base_df[["Question id", "Correct"]].copy()  # <-- BASELINE_CSV: Question id, Correct

# new 只留：Question id + Correct
new_keep = new_df[["Question id", "Correct"]].copy()  # <-- NEW_CSV: Question id, Correct

# 把 canonical 标签 merge 到 baseline
# 得到：Question id, Correct, Subdomain_question, QuestionType
base_labeled = base_keep.merge(cat_df, on="Question id", how="left")  # <-- CATEGORY_CSV 加到 BASELINE

# 把 canonical 标签 merge 到 new
new_labeled = new_keep.merge(cat_df, on="Question id", how="left")  # <-- CATEGORY_CSV 加到 NEW

# 以 (Question id, Subdomain_question, QuestionType) 为键合并 baseline vs new
# 得到：Correct_base, Correct_new
m = base_labeled.merge(
    new_labeled,
    on=["Question id", "Subdomain_question", "QuestionType"],
    suffixes=("_base", "_new")
)

# 将 Correct 列转为 bool（确保均值=准确率）
m["Correct_base"] = m["Correct_base"].astype(bool)  # <-- BASELINE Correct
m["Correct_new"] = m["Correct_new"].astype(bool)    # <-- NEW Correct


# =========================
# 4) 论文口径过滤：只保留 9 个规范子领域（>=3 题）
# =========================

# 统计每个 Subdomain_question 的题数（用 CATEGORY_CSV 的标签口径）
sub_counts = cat_df["Subdomain_question"].value_counts()

# 过滤：只保留题数 >= 3 的子领域
# （这会自动排除你论文提到的异常“Algebraic Calculation”子领域 2 题）
valid_subdomains = sub_counts[sub_counts >= 3].index.tolist()

# 在合并后的数据中只保留这些规范子领域
m = m[m["Subdomain_question"].isin(valid_subdomains)].copy()

# 采用论文里 Fig3 的子领域顺序（如果缺失则自动跳过）
canonical_order = [
    "Quantum Computation",
    "Nuclear Physics",
    "Quantum Mechanics",
    "Quantum Chemistry",
    "Mathematics",
    "Quantum Field Theory",
    "Optics",
    "String Theory",
    "Photonics",
]
canonical_order = [sd for sd in canonical_order if sd in valid_subdomains]  # 防御性处理


# =========================
# 5) (a) 子领域 Δpp + paired bootstrap 95% CI
# =========================

# 使用固定随机种子，保证 CI 可复现（若你论文用不同种子/次数可改这里）
rng = np.random.default_rng(0)

def bootstrap_delta_ci(df_sub, B=4000):
    """
    对一个子领域做 paired bootstrap:
    - 每道题都有 (new_correct - base_correct) ∈ {-1,0,1}
    - Δpp = mean(d) * 100
    - CI = bootstrap percentile [2.5,97.5]
    """
    x = df_sub["Correct_new"].to_numpy(dtype=int)   # <-- NEW Correct
    y = df_sub["Correct_base"].to_numpy(dtype=int)  # <-- BASELINE Correct
    d = x - y                                       # paired difference
    n = len(d)

    # 点估计：Δpp
    delta = d.mean() * 100.0

    # bootstrap 重采样（有放回抽题目）
    idx = rng.integers(0, n, size=(B, n))
    boot = d[idx].mean(axis=1) * 100.0
    lo, hi = np.percentile(boot, [2.5, 97.5])

    return delta, lo, hi

# 逐子领域计算：Δpp, CI, baseline acc, 题量 N
delta_pp = []       # <-- (a)(b)(d) 都会用到：y 值
ci_low = []         # <-- (a) 画 CI
ci_high = []        # <-- (a) 画 CI
n_questions = []    # <-- (b) bubble size
baseline_acc = []   # <-- (b) x 值

for sd in canonical_order:
    df_sd = m[m["Subdomain_question"] == sd]  # <-- 使用 CATEGORY_CSV 的 Subdomain_question 分组

    n_questions.append(len(df_sd))  # <-- 该子领域题量
    baseline_acc.append(df_sd["Correct_base"].mean() * 100.0)  # <-- baseline accuracy (%)

    d, lo, hi = bootstrap_delta_ci(df_sd, B=4000)
    delta_pp.append(d)
    ci_low.append(lo)
    ci_high.append(hi)

delta_pp = np.array(delta_pp)
ci_low = np.array(ci_low)
ci_high = np.array(ci_high)
n_questions = np.array(n_questions)
baseline_acc = np.array(baseline_acc)


# =========================
# 6) (c) Pareto：子领域对 new 的正确贡献（# Correct new + cumulative %）
# =========================

# 每个子领域 new 的正确数（Correct_new=True 计数）
new_correct_counts = (
    m.groupby("Subdomain_question")["Correct_new"]
     .sum()
     .reindex(canonical_order)
     .astype(int)
)

# 按 new correct 数降序排列（Pareto）
order = np.argsort(new_correct_counts.to_numpy())[::-1]
sub_sorted = np.array(canonical_order)[order]
counts_sorted = new_correct_counts.to_numpy()[order]
cum_pct_sorted = np.cumsum(counts_sorted) / max(1, counts_sorted.sum()) * 100.0


# =========================
# 7) (d) Heatmap：题型 × 子领域 的 Δpp（pp）
# =========================

# 论文题型：三类（来自 CATEGORY_CSV 的 QuestionType 列）
qtypes = ["Conceptual Understanding", "Numerical Calculation", "Algebraic Calculation"]

# 显示用短名字（仅用于坐标轴显示）
qtype_disp = {
    "Conceptual Understanding": "Conceptual",
    "Numerical Calculation": "Numerical",
    "Algebraic Calculation": "Algebraic",
}

# heatmap 数值矩阵：shape = (3, 9)
# 每个格子=该题型×子领域的 Δpp（new_acc - base_acc）
delta_mat = np.full((len(qtypes), len(canonical_order)), np.nan)

for i, qt in enumerate(qtypes):
    for j, sd in enumerate(canonical_order):
        cell = m[(m["QuestionType"] == qt) & (m["Subdomain_question"] == sd)]  # <-- CATEGORY_CSV: QuestionType/Subdomain

        # 论文里通常不希望极小样本格子误导，所以设阈值：>=3 才计算
        if len(cell) >= 3:
            delta_mat[i, j] = (cell["Correct_new"].mean() - cell["Correct_base"].mean()) * 100.0


# =========================
# 8) 画图：4-panel layout（避免重叠 + (a) 丰富配色）
# =========================

# 字体嵌入：保证 PDF 在 LaTeX/投稿系统中更稳（TrueType）
plt.rcParams.update({
    "pdf.fonttype": 42,
    "ps.fonttype": 42,
})

# 整体更宽：给 (c)(d) x tick 留空间，避免重叠
fig = plt.figure(figsize=(11.4, 8.8))

# GridSpec：3 行 2 列
# - 第 0 行：a 横跨两列
# - 第 1 行：b 横跨两列
# - 第 2 行：c 左，d 右
gs = GridSpec(
    nrows=3, ncols=2,
    height_ratios=[1.0, 1.0, 1.25],
    hspace=0.60,
    wspace=0.55,
    figure=fig
)

ax_a = fig.add_subplot(gs[0, :])
ax_b = fig.add_subplot(gs[1, :])
ax_c = fig.add_subplot(gs[2, 0])
ax_d = fig.add_subplot(gs[2, 1])

# 全局边距：bottom 必须大（旋转 tick 会占空间）
fig.subplots_adjust(left=0.10, right=0.98, top=0.95, bottom=0.22)

# ---------- (a) ----------
# 丰富配色：每个 subdomain 一个颜色（tab10），CI 同色淡化
palette = sns.color_palette("tab10", n_colors=len(canonical_order))

y = np.arange(len(canonical_order))

# 画 CI（淡色线）
for i in range(len(canonical_order)):
    ax_a.hlines(
        y=i,
        xmin=ci_low[i],
        xmax=ci_high[i],
        color=palette[i],
        lw=2,
        alpha=0.35
    )

# 画点（实色）
ax_a.scatter(delta_pp, y, c=palette, s=35, zorder=3)

# 0 参考线
ax_a.axvline(0, color="steelblue", lw=1)

# 标注 Δpp 数值（避免覆盖点：正向右偏移，负向左偏移）
for i, v in enumerate(delta_pp):
    ax_a.text(
        v + (0.55 if v >= 0 else -0.55),
        i,
        f"{v:+.2f}",
        va="center",
        ha="left" if v >= 0 else "right",
        fontsize=9,
        clip_on=False
    )

# ytick：子领域
ax_a.set_yticks(y)
ax_a.set_yticklabels(canonical_order)
ax_a.invert_yaxis()
ax_a.set_title("(a) Subdomain gains/losses with bootstrap 95% CI (paired)")
ax_a.set_xlabel("")  # 论文排版里通常不在 (a) 写 x label（减少拥挤）

# ---------- (b) ----------
# bubble size：∝ 子领域题数（真实数据：n_questions）
sizes = (n_questions * 10).clip(30, None)

ax_b.scatter(baseline_acc, delta_pp, s=sizes, alpha=0.7)
ax_b.axhline(0, color="steelblue", lw=1)

# 标注：只标注题量较大的子领域，避免文字拥挤
for i, name in enumerate(canonical_order):
    if n_questions[i] >= np.percentile(n_questions, 60):
        ax_b.text(baseline_acc[i] + 0.4, delta_pp[i], name, fontsize=9)

ax_b.set_xlabel("Baseline accuracy (%)")
ax_b.set_ylabel("Δ Accuracy (pp)")
ax_b.set_title("(b) Gain vs baseline accuracy (bubble size ∝ #questions)")

# ---------- (c) ----------
x = np.arange(len(sub_sorted))

ax_c.bar(x, counts_sorted, color="#4C72B0")

ax_c_twin = ax_c.twinx()
ax_c_twin.plot(x, cum_pct_sorted, color="black", marker="o", lw=1.5)

ax_c.set_xticks(x)
ax_c.set_xticklabels(
    wrap_labels(sub_sorted, width=14),
    rotation=35,
    ha="right",
    fontsize=8
)
ax_c.tick_params(axis="x", pad=2)
ax_c.set_ylabel("# Correct (new)")
ax_c_twin.set_ylabel("Cumulative (%)", labelpad=6)
ax_c.set_title("(c) Pareto: subdomain contribution")

# ---------- (d) ----------
sns.heatmap(
    delta_mat,
    ax=ax_d,
    cmap="viridis",
    cbar=True,
    xticklabels=wrap_labels(canonical_order, width=14),
    yticklabels=[qtype_disp[q] for q in qtypes],
    annot=False,
    # 色条范围：根据数据自适应（若你想固定可手动改 vmin/vmax）
    vmin=np.nanmin(delta_mat),
    vmax=np.nanmax(delta_mat),
    cbar_kws={"shrink": 0.95, "pad": 0.02}
)

ax_d.set_title("(d) Type × subdomain interaction (Δpp)")
ax_d.set_xlabel("")
ax_d.set_ylabel("")
ax_d.set_xticklabels(ax_d.get_xticklabels(), rotation=35, ha="right", fontsize=8)
ax_d.set_yticklabels(ax_d.get_yticklabels(), rotation=0)


# =========================
# 9) 输出（PNG + PDF 的格式要求）
# =========================

# 【PNG 输出要求】
# - dpi=300：保证论文/补充材料清晰
# - bbox_inches="tight"：自动裁掉多余空白（避免 LaTeX 里出现大边）
# - pad_inches=0.02：留一点安全边距（避免文字贴边被裁）
plt.savefig(
    f"{OUT_PREFIX}.png",
    dpi=300,                      # <-- PNG 分辨率要求
    bbox_inches="tight",          # <-- 重要：避免裁切/重叠后被截断
    pad_inches=0.02               # <-- 重要：防止贴边文字被裁
)

# 【PDF 输出要求】
# - 同样 bbox tight + pad
# - pdf.fonttype=42 已在上面设置：TrueType 字体嵌入，LaTeX/投稿系统更稳
plt.savefig(
    f"{OUT_PREFIX}.pdf",
    bbox_inches="tight",          # <-- PDF 裁边要求
    pad_inches=0.02               # <-- PDF 安全边距
)

plt.close(fig)  # 关闭图像，防止批量绘图时内存增长
